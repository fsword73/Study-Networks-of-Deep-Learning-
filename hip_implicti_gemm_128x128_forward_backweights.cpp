2019-04-18 

#include <assert.h>
#include <stdio.h>
#include <algorithm>
#include <stdlib.h>
#include<iostream>
#include "hip/hip_runtime.h"


#define HIP_ASSERT(x) (assert((x)==hipSuccess))


__global__ void 
static_compiled_gemm_128x128_forward(hipLaunchParm lp,
		const float* __restrict__ input, const float* __restrict__ weights, float* __restrict__ output,
		const int* unified_offset, const int* input_perthread_offset, const int* output_per_thread_offset,
		int  N, int C,  int H, int W, int K, int R, int S, int Padding_X, int Padding_Y,
		int  out_H, int out_W)
{
	//Algorithm
	//  N  BatchSize 
	//	C  Input Channels
	//  H  Input Height 
	//  W  Input Weight 
	//  K  Output Channels 
	//  R  FilterSize Horizontal
	//  S  FilterSize Veritical 
	//  U  Stride-X  
	//  V  Stride-Y  
	//  P  Padding_X 
	//  Q  Padding_Y
	
	// unified_offset:        generated by CPU code 
	// input_thread_Offset:  generated by CPU code  
	// output_thread_Offset
	//Mapping Input Matrix:
	//     M = N * H * W 
	//     K = R * S * C  
	//     N = K 
	
	//Macro 	Tile Size: 128x128
	//Wave  	Tile Size  128x32 
	//Thread 	Tile Size  8x8 
	
	//2-D workgroup
	//Global workgorup size:  [N * output_H * output_W ,  K, 1] 
	

	//This implementation uses 1-D workgroup 	
	
	//1-D workgroup
	//Global workgorup size = [N * output_H * output_W+127]/128 * [K+127]/128;
	
	
	int x = hipBlockDim_x ;
	
	int BLOCKS_PER_K = (K+127)/128; 
	
	int K_offset  = (hipBlockDim_x % BLOCKS_PER_K) * 128; 
	int M_offset  = (hipBlockDim_x / BLOCKS_PER_K) * 128; 	
	
	
	
	float sum[8][8];
	for(int i=0; i < 8; i++)
		for(int j=0; j < 8; j++)
			sum[i][j]=0;

	//double buffer load 
	__shared__ float shared_matrix_A[8*2][129];
	__shared__ float shared_matrix_B[8*2][129];
	
	for(int i=0; i < 8; i++)
	{
			int index  = hipThreadIdx_x/128;
			int offset = hipThreadIdx_x%128;
			shared_matrix_A[(hipThreadIdx_x/128)*8+i][hipThreadIdx_x%128]=0;
			shared_matrix_B[(hipThreadIdx_x/128)*8+i][hipThreadIdx_x%128]=0;
	}
	
	#Preload 1st K=8 
	#Matrix_A, 128x8 floats by first 128 threads
	#Matrix_B, 128x8 floats by next  128 threads
	
	float input_data[8];	
	
	const float* input_data_base=0;	
	
	int x = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;
	if(hipThreadIdx_x < 128)
	{
		input_data_base = input + input_perthread_offset[x];	
		//Non-128 aligned NHW is dealed by input_perthread_offset[x]
		
		//In C, i will be 
		//   if ( M_Offset + (hipThreadIdx_x%128) >= (N*out_H*out_W)
		//		M_Offset = N* out_H * N_out_W -1;
		//    The reason is that N*H*W in 1-Dimension	
		//    All out of boundary N*H*W will be last pixel
		//
		//
	}
	else
	{			
		//Add code to deal with K not =128xn 
		int kk_offset = K_offset + (hipThreadIdx_x%128); 
		if (kk_offset >= K)
			kk_offset = (K-1);
		
		input_data_base = weights + (kk_offset) * C * R * S;
	}
	int unified_offset_k8[8]; 
	int offset_in_CxRxS = 0;
	if(hipThreadIdx_x < 128)
	{
		for(int i=0; i < 8; i++)
		{
			
			[i] = unified_offset[offset_in_CxRxS + i];
		}
	}
	else
	{
		for(int i=0; i < 8; i++)
		{
			
			unified_offset[i] = offset_in_CxRxS + i;
		}
	}
	offset_CxRxS += 8;
	for(int i=0; i < 8; i++)
	{
		input_data[i] = input_data_base[unified_offset[i]];
	}
	
	int lds_write_group = 0;
	
	
	if(hipThreadIdx_x < 128)
	{
		for(int i=0; i < 8; i++)
		{
				shared_matrix_A[lds_write_group*8+i] = input_data[i];
		}
	}
	else
	{
		for(int i=0; i < 8; i++)
		{
				shared_matrix_B[lds_write_group*8+i] = input_data[i];
		}
	}
	lds_write_group = (lds_write_group+1)%2; 
	
	__syncthreads();
	
	float data_a[8];
	float data_b[8];
	
	//each threads' m,n offset
	int mm_offset;  
	int mm_offset;
	
	mm_offset = (hipThreadIdx_x%16)*8;
	nn_offset = (hipThreadIdx_x/16)*8;
	
	
	//major loop
	//unroll matrix K = 8
	for(int kk=0; kk < (C*R*S); kk+=8)
	{
		//global Load 
		if(	kk < ( C*R*S-8))
		{
			if(hipThreadIdx_x < 128)
			{
				for(int i=0; i < 8; i++)
				{
					
					unified_offset_k8[i] = unified_offset[offset_in_CxRxS + i];
				}
			}
			else
			{
				for(int i=0; i < 8; i++)
				{
					
					unified_offset[i] = offset_in_CxRxS + i;
				}
			}
			offset_in_CxRxS += 8;	
			for(int i=0; i < 8; i++)
			{
				input_data[i] = input_data_base[unified_offset[i]];
			}
		}
		
		//DO FMA 8x8x8 
		for(int kkk=0; kkk < 8; kkk++)
		{
			for(int mm=0; mm < 8; mm++)
			{
				int lds_read_group = (lds_write_group+1)%2;
				data_a[mm] = shared_matrix_A[lds_read_group*8+kk][mm_offset+mm]);	
				
			}
			for(int nn=0; nn < 8; nn++)
			{
				int lds_read_group = (lds_write_group+1)%2;
				data_b[nn] = shared_matrix_A[lds_read_group*8+kk][nn_offset+nn]);					
			}
			
			for(int mm=0; mm < 8; mm++)
				for(int nn=0; nn<8; nn++)
					sum[mm][nn] += 	data_a[mm] * data_b[nn];	
		}

		//LDS write 
		if(	kk < ( C*R*S-8)){
			if(hipThreadIdx_x < 128)
			{
				for(int i=0; i < 8; i++)
				{
						shared_matrix_A[lds_write_group*8+i] = input_data[i];
				}
			}
			else
			{
				for(int i=0; i < 8; i++)
				{
						shared_matrix_B[lds_write_group*8+i] = input_data[i];
				}
			}
			lds_write_group = (lds_write_group+1)%2; 
			
			__syncthreads();	
		}			
	}		

	//Output... we need to mask off 	

	//To Add K-Unroll != 8 
	// (C * R * S) &(0x7)
		
	
	for( int i=0; i< 8; i++)
		for( int j=0; j < 8; j++)
		{
			int nhw 	= M_offset + mm_offset + i;
			int out_k 	= K_offset + nn_offset + j;
			
			int n = nhw / (H * W);
			int h = (nhw % (H * W)) /W;
			int w = (nhw % (H * W)) %W;
			
			float* ouput_base = output + n * K * out_H * out_W +  out_k * out_H * out_W + h * out_W + w;	

			if( n < N && out_k < K)
			{
				*ouput_base = sum[i][j];
			}				
		}	
}


__global__ void 
static_compiled_gemm_128x128_backweights(hipLaunchParm lp,
		const float* __restrict__ inputs, float* __restrict__ gradOutput, const float* __restrict__ gradWeights,
		float learningMultiplier
		const int* unified_offset_input, const int* unified_offset_output,
		const int* input_perthread_offset, const int* output_per_thread_offset,
		int  N, int C,  int H, int W, int K, int R, int S, int Padding_X, int Padding_Y,
		int  out_H, int out_W)
{
	//Algorithm
	//  N  BatchSize 
	//	C  Input Channels
	//  H  Input Height 
	//  W  Input Weight 
	//  K  Output Channels 
	//  R  FilterSize Horizontal
	//  S  FilterSize Veritical 
	//  U  Stride-X  
	//  V  Stride-Y  
	//  P  Padding_X 
	//  Q  Padding_Y

	//Convert into GEMM output
		//matrix M = C * R* S 
		//matrix N = K 
		//matrix K = N * out_H * out_W


		//Padding is very hard to do in fetching
		//Assuming padding is done copy function.
		//inputs has Padding inside.

		//Global work group size in 1-Dimension
		//Global work size in 1-D == [((K+127)/128)*128]* [ ((C * R * S+127)/128)*128] 
		
		
		//Macro-Tile-Size for K*C*R*S 128*128
		//Wave-tile-size  for K*C*R*S 128*32
		//Threadtile-size  for K*C*R*S 8x8
		
		//Cacheline Reuse 
		//For example,  R=1 x S=7    
		//total L1 = 16 KBytes,  or 256 Cachelines in total 
		//M = 128 Means 128/7 = 19 cachelines		
		//These 19 cachlines will be used for  for next 16 matrix_K from  NHW 
		//However K =128 needs 128 Cachlelines 
		//2-workgrooups needs 256 Cachelines + 38 Cachelines 
		//which is bigger than total L1 Cacheline size 256
		//Considering Worst case will be 128 Cachelines for 128 M value or 1x1 Cases				
		//In other words,  macro tile size 64x64 will be much better for 1x1 cases. 
		//1 workgroup needs 128 Cachelines for 1x1,  
		//16 Kbytes L1 works well for 2 workgproups 
		//
		
		//For Cases, NHW will be very huge, for example 35x35x64
		//C,K is very small , for example, 256x256 
		//even macro tile size 64x64 has only 16 wkgroups. 
		//We need to split the workload into 4 segments or 8 segments to have 128 workgroups
		//Even 256 workgroups by Split NHW to 16 segments
		//Using Float-Atomic to Complete the final reduction
		//35x35x4 = 4900 is still very huge 	
		//to have better performance for K=8 unroll 
		//
		//Thhere are 2 mthods:
		//  1) 35x35x4 = 4900 --> 4904, So that the last K split will have 64 less  K 
		//  2) First Execution + 60K,  All other rest 15 Split will reduce SPlit-K by 4
		
		//Considering the C, K, R, S range.  
		// 128x128, 64x64, 64x32, 64x16, 64x8 
		// 32x32, 16x16, 8x8 	
		// Plus unroll K!=8 
		// + Split-K 
		
		int BLOCKS_PER_K =    (K+127)/128;
		int matrix_n_offset = (hipBlockDim_x % BLOCKS_PER_K)*128;
		int matrix_m_offset = (hipBlockDim_x / BLOCKS_PER_K)*128;  
		
		
		float sum[8]98];
		for(int i=0; i < 8; i++)
			for(int(j=0; j < 8; j++)
				sum[i][j] = 0;
		
		//double buffer load 
		__shared__ float shared_matrix_A[8*2][129];
		__shared__ float shared_matrix_B[8*2][129];
		for(int i=0; i < 8; i++)
		{
				int index  = hipThreadIdx_x/128;
				int offset = hipThreadIdx_x%128;
				shared_matrix_A[(hipThreadIdx_x/128)*8+i][hipThreadIdx_x%128]=0;
				shared_matrix_B[(hipThreadIdx_x/128)*8+i][hipThreadIdx_x%128]=0;
		}

		//per-thread  base offset for first 128 threads; 

		int wg_c_global_fetch = 0; 
		int wg_r_global_fetch = 0;
		int wg_s_global_fetch = 0;
		int k_global_fetch = 0;
		int matrix_a_offset = 0;
		int matrix_b_offset = 0;

		//Thread 0-127 		load Matrix A
		//Thread 128-255 	load Matrix B
		if( hipThreadIdx_x < 128)
		{
			int m_offset_global_fetch = matrix_m_offset + hipThreadIdx_x;
			wg_c_global_fetch = m_offset_global_fetch / (R*S);
			wg_r_global_fetch = (m_offset_global_fetch % (R*S) ) / S;			
			wg_s_global_fetch = (m_offset_global_fetch % (R*S) ) * S;			
			matrix_a_offset = wg_c_global_fetch * H * W + wg_r_global_fetch * W + wg_s_global_fetch;
			//IF C *R*S is out of boundary
			if(wg_c_global_fetch >=C)
				matrix_a_offset = C* H * W -1;
		}
		else
		{			
			k_global_fetch = matrix_n_offset + (hipThreadIdx_x%128);
			
			matrix_b_offset = k_global_fetch * out_H * out_W;
			
			// if K is out of boundary
			if(k_global_fetch >= K)
				matrix_b_offset = K * out_H * out_W -1;
		}

		float matrix_a[8];
		float matrix_b[8];
		
		int unified_offset_k8[8];
		int matrix_k_index = 0;
		
		//Following code can be generated by CPU to load Constant buffer
		//unified_offset_input & unified_offset_output	
		if( hipThreadIdx_x < 128)
		{
			for(int i=0;  i < 8; i++)
			{
				int n =  matrix_k_index / ( out_H*out_W);
				int h = (matrix_k_index % ( out_H*out_W)) / out_W;
				int w = (matrix_k_index % ( out_H*out_W)) % out_W;
				
				unified_offset_k8[i] = n * C * H * W + h * W + w;
				matrix_k_index++;
			}
		}
		else		
		{
			for(int i=0;  i < 8; i++)
			{
				int n =  matrix_k_index / ( out_H*out_W);
				int h = (matrix_k_index % ( out_H*out_W)) / out_W;
				int w = (matrix_k_index % ( out_H*out_W)) % out_W;
				
				unified_offset_k8[i] = n * K * out_H * out_W + h * out_W + w;
				matrix_k_index++;
			}			
		}
		
		//Pretch K=8
		if( hipThreadIdx_x < 128)
		{
			for( int i = 0; i < 8; i++)
			{
				matrix_a[i] = inputs[matrix_a_offset +unified_offset_k8[i]];	
			}
		}		
		else
		{
			for( int i = 0; i < 8; i++)
			{
				matrix_b[i] = gradOutput[matrix_a_offset +unified_offset_k8[i]];	
			}
		}
		
		int lds_write_group = 0;
	
	
		if(hipThreadIdx_x < 128)
		{
			for(int i=0; i < 8; i++)
			{
					shared_matrix_A[lds_write_group*8+i] = matrix_a[i];
			}
		}
		else
		{
			for(int i=0; i < 8; i++)
			{
					shared_matrix_B[lds_write_group*8+i] = matrix_b[i];
			}
		}
		lds_write_group = (lds_write_group+1)%2;
		__syncthreads();			
		
		
		//each threads' m,n offset
		int mm_offset;  
		int mm_offset;
	
		mm_offset = (hipThreadIdx_x%16)*8;
		nn_offset = (hipThreadIdx_x/16)*8;
		//DO FMA 8x8x8 
		for(int kk=0; kk < (N*out_H*out_W); kk+=8)
		{
			//Load Data
			if( kk < ( N*out_H*out_W -8))
			{
				if( hipThreadIdx_x < 128)
				{
					for(int i=0;  i < 8; i++)
					{
						int n =  matrix_k_index / ( out_H*out_W);
						int h = (matrix_k_index % ( out_H*out_W)) / out_W;
						int w = (matrix_k_index % ( out_H*out_W)) % out_W;
						
						unified_offset_k8[i] = n * C * H * W + h * W + w;
						matrix_k_index++;
					}
				}
				else		
				{
					for(int i=0;  i < 8; i++)
					{
						int n =  matrix_k_index / ( out_H*out_W);
						int h = (matrix_k_index % ( out_H*out_W)) / out_W;
						int w = (matrix_k_index % ( out_H*out_W)) % out_W;
						
						unified_offset_k8[i] = n * K * out_H * out_W + h * out_W + w;
						matrix_k_index++;
					}			
				}
				
				//Pretch K=8
				if( hipThreadIdx_x < 128)
				{
					for( int i = 0; i < 8; i++)
					{
						matrix_a[i] = inputs[matrix_a_offset +unified_offset_k8[i]];	
					}
				}		
				else
				{
					for( int i = 0; i < 8; i++)
					{
						matrix_b[i] = gradOutput[matrix_a_offset +unified_offset_k8[i]];	
					}
				}
			}
			
			//Do 8x8x8 FMA
			for(int kkk=0; kkk < 8; kkk++)
			{
				for(int mm=0; mm < 8; mm++)
				{
					int lds_read_group = (lds_write_group+1)%2;
					data_a[mm] = shared_matrix_A[lds_read_group*8+kk][mm_offset+mm]);	
					
				}
				for(int nn=0; nn < 8; nn++)
				{
					int lds_read_group = (lds_write_group+1)%2;
					data_b[nn] = shared_matrix_A[lds_read_group*8+kk][nn_offset+nn]);					
				}
				
				for(int mm=0; mm < 8; mm++)
					for(int nn=0; nn<8; nn++)
						sum[mm][nn] += 	data_a[mm] * data_b[nn];	
			}
			
			if( kk < ( N*out_H*out_W -8))
			{
				if(hipThreadIdx_x < 128)
				{
					for(int i=0; i < 8; i++)
					{
							shared_matrix_A[lds_write_group*8+i] = matrix_a[i];
					}
				}
				else
				{
					for(int i=0; i < 8; i++)
					{
							shared_matrix_B[lds_write_group*8+i] = matrix_b[i];
					}
				}
				lds_write_group = (lds_write_group+1)%2;
				__syncthreads();
			}
		}
		
		//To Add K-Unroll != 8 
		// (N * out_H * out_W) &(0x7)
		
		//Write out final weights
		if( (matrix_n_offset+128) <= K && (matrix_n_offset + 128) <= (C*R*S))
		{
			for(int i =0; i < 8; i++)			
			{
				for(int j =0; j < 8; j++)
				{
					int m = M_Offset + mm_offset;
					int k = K_offset + nn_offset;
					
					int c = m / (R*S);
					int r = (m % (R*S)) / S;
					int s = (m % (R*S)) % S;
					
					gradWeights[ k * C * R *S + r *S + s] = sum[i][j]* learningMultiplier;
				}
			}
		}
		else
		{
			for(int i =0; i < 8; i++)			
			{
				for(int j =0; j < 8; j++)
				{
					int m = M_Offset + mm_offset;
					int k = K_offset + nn_offset;
					
					int c = m / (R*S);
					int r = (m % (R*S)) / S;
					int s = (m % (R*S)) % S;
					
					//Do not output if Non-128 aligned. 
					if( c < C && K < k)
						gradWeights[ k * C * R *S + r *S + s] = sum[i][j]* learningMultiplier;
				}
			}
		}
		
}
